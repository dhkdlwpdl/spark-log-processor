/*
 * This source file was generated by the Gradle 'init' task
 */
package com.example
import org.apache.spark.sql.{SparkSession, functions => F}

object App {
  def main(args: Array[String]): Unit = {
    // Original CSV 파일 경로
    val inputCsvPath = "tmp/original_data/2019-Nov-Sample.csv"
    // 결과 Parquet 파일 저장 경로
    val outputParquetPath = f"file:///Users/admin/Desktop/homework/spark-log-processor/tmp/processed_data"
    val checkpointPath = "tmp/checkpoint"
    val targetTableName = "test_table"

    val spark = SparkSession.builder()
      .appName("LogProcessor")
      .config("spark.sql.shuffle.partitions", "2")
      .master("local[*]")
      .enableHiveSupport()
      .getOrCreate()

//    spark.sql(f"drop table if exists $targetTableName")


     // 체크포인트 디렉토리 설정 (장애 발생 시 복구를 위한 디렉토리)
     spark.sparkContext.setCheckpointDir(checkpointPath)

     // 마지막 처리된 timestamp 추적 (예: 메타데이터에서 읽어와야 함)
     val lastProcessedTimestamp = "2023-01-01T00:00:00Z"  // 이 값을 실제 메타데이터에서 가져옵니다.

     // CSV 파일 읽기
     val df = spark.read
       .option("header", "true")  // 첫 번째 행을 헤더로 처리
       .option("inferSchema", "true")  // 데이터의 타입을 자동으로 추론
       .csv(inputCsvPath)
//       .filter(F.col("event_time").geq(lastProcessedTimestamp))  // timestamp 기준 필터링 (추가 기간 처리)

     // 체크포인트 설정 (중간 상태를 디스크에 저장)
     df.checkpoint()

     println(s"Row count: ${df.count()}")  // 행 수 확인

     // Parquet 형식으로 저장 (Snappy 압축)
     val writeData = df.write
       .format("parquet")
       .option("compression", "snappy")  // Snappy 압축
       .option("path", outputParquetPath)

    if (!spark.catalog.tableExists(targetTableName)) {
      writeData.saveAsTable(targetTableName)  // 테이블이 없으면 생성 후 저장
    } else {
      writeData.mode("append").save()  // 테이블이 있으면 append 모드로 저장
    }

     spark.sql(f"select count(*) from $targetTableName").show()
     spark.stop()
  }

}
